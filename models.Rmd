---
title: "Polices d'assurance"
author: "Idriss Louzi, Martin Youssef, Alex Irani, Théophile Schmutz"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    
    # Overall theme
    theme: flatly
    highlight: tango
    code_folding: show
    
    # Table of contents
    number_sections: true
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(rmarkdown)
library(dplyr)
library(caret)
library(kableExtra)
library(ggplot2)
library(broom)
library(tidyr)
library(GGally)
library(glmnet)
library(rcompanion)
library(reshape2)
library(caret)
library(MASS)
library(Metrics)
library(tibble)

set.seed(2025)
rmse_c <- function(actuals, predictions, print=TRUE) {
  # Définition des classes
  C_1 <- which(actuals %in% c(0, 1))
  C_2 <- which(actuals == 2)
  C_3 <- which(actuals == 3)
  C_4 <- which(actuals > 3)
  
  # Calcul du RMSE pour chaque classe (en évitant les erreurs si une classe est vide)
  rmse_1 <- rmse(actuals[C_1], predictions[C_1]) 
  rmse_2 <- rmse(actuals[C_2], predictions[C_2]) 
  rmse_3 <- rmse(actuals[C_3], predictions[C_3]) 
  rmse_4 <- rmse(actuals[C_4], predictions[C_4]) 
  
  # Combinaison des RMSE (en ignorant les valeurs NA)
  rmse_values <- c(rmse_1, rmse_2, rmse_3, rmse_4)
  RMSE_C <- mean(rmse_values, na.rm = TRUE)  # Moyenne des RMSE valides
  
  if (print){
    # Affichage des résultats
    cat("RMSE_1 (classe très fréquente) :", rmse_1, "\n")
    cat("RMSE_2 (classe fréquente) :", rmse_2,"-",length(C_2), "observations\n")
    cat("RMSE_3 (classe rare) :", rmse_3,"-",length(C_3), "observations\n")
    cat("RMSE_4 (classe très rare) :", rmse_4,"-",length(C_4), "observations\n")
    cat("RMSE combiné (RMSE_C) :", RMSE_C, "\n")}
  
  return(RMSE_C)
}
custom <- trainControl(
  method = 'repeatedcv',
  number = 5,  # Using 5-fold cross-validation
  repeats = 3,  # Repeating 3 times for robustness
  summaryFunction = rmse_c, #defaultSummary,  # Default metrics (RMSE, MAE)
  allowParallel = TRUE  # Use parallel processing if resources allow
)
```

# Données

```{r}
data <-read.csv('train_set.csv', header = T, sep = ",",dec=".")
data <- data %>% dplyr::select(-PolID)
paged_table(data)
```

```{r}
data$TrancheAge <- cut(data$Age, 
                        breaks = c(-Inf, 25, 35, 45, 55, 65, 110), 
                        labels = c("Moins de 25 ans", "25-34 ans", "35-44 ans", 
                                   "45-54 ans", "55-64 ans", "65 ans et plus"), 
                        right = FALSE)
data <- data %>%
  mutate(TrancheBonus_Malus = case_when(
    Bonus_Malus < 100 ~ "Bonus",
    Bonus_Malus == 100 ~ "Neutre",         # 100 = pas de bonus, pas de malus
    Bonus_Malus > 100 & Bonus_Malus <= 150 ~ "Malus modéré",
    Bonus_Malus > 150 & Bonus_Malus <= 250 ~ "Malus élevé",
    Bonus_Malus > 250 & Bonus_Malus <= 350 ~ "Malus très élevé",
    TRUE ~ "Erreur" 
  ))

data <- data %>%
  mutate(TrancheCar_Power = case_when(
    Car_Power <= 6 ~ "Puissance --",
    Car_Power > 6 & Car_Power <= 10 ~ "Puissance -",
    Car_Power > 10 & Car_Power <= 12 ~ "Puissance +",
    Car_Power > 12 ~ "Puissance ++",
    TRUE ~ "Erreur" 
  ))

data[, c("Car_Model", "Car_Fuel", "Urban_rural_class", "French_region", "TrancheCar_Power", "TrancheBonus_Malus", "TrancheAge")] <- lapply(data[, c("Car_Model", "Car_Fuel", "Urban_rural_class", "French_region", "TrancheCar_Power", "TrancheBonus_Malus", "TrancheAge")], as.factor)
```

# Modélisation

On split en train test avec stratification selon les classes du RMSE combiné 
```{r}
data$Claim <- as.numeric(data$Claim)
data <- data %>%
  mutate(Ci = case_when(
    Claim %in% c(0, 1) ~ 1,
    Claim == 2 ~ 2,
    Claim == 3 ~ 3,
    Claim >= 4 ~ 4,
  ))
data$Ci <- as.factor(data$Ci)
train.index <- createDataPartition(data$Ci, p = .6, list = FALSE)
data <- data %>% dplyr::select(-Ci)

train <- data[ train.index,] 
test  <- data[-train.index,]
x_train <- train %>% 
  dplyr::select(-Claim) %>%
  as.matrix()
y_train <- train$Claim

x_test <- test %>% 
  dplyr::select(-Claim) %>%
  as.matrix()
y_test <- test$Claim
```


## Model A: Simple case

Avec simplement les données comme tel:
```{r}
model.complet <- lm(Claim~., data=train)
summary(model.complet)$coefficients %>%
  round(4) %>%
  kbl() %>%
  kable_styling(full_width = FALSE,)
```

```{r}
y_pred <- predict(model.complet, test %>%  dplyr::select(-Claim))
metrics_complet <- tibble(
  RMSE_C = rmse_c(y_test, y_pred),
  RMSE = rmse(y_test, y_pred),
  R2 = R2(y_pred, y_test),
  MSE = mean((y_test - y_pred)^2),
)

metrics_complet %>%
  kbl(digits = 3) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```

```{r}
results <- data.frame(
  "pred" = y_pred,
  "actual" = y_test,
  "status" = y_test == round(y_pred)
)

ggplot(results, aes(x = actual, y = pred)) +
  geom_point(aes(color = status)) +
  labs(
    x = "Actual",
    y = "Prediction",
    title = "Répartitions des erreurs"
  ) +
  theme_minimal()
```
On prédit que la classe majoritaire C0.

### Model bis: Poids de classe

```{r}
C_1 <- which(train$Claim %in% c(0, 1))
C_2 <- which(train$Claim == 2)
C_3 <- which(train$Claim == 3)
C_4 <- which(train$Claim > 3)

n1 <- length(which(train$Claim %in% c(0, 1)))
n2 <- length(which(train$Claim == 2))
n3 <- length(which(train$Claim == 3))
n4 <- length(which(train$Claim > 3))

# Calculer les poids pour équilibrer l'importance des classes
w <- numeric(length(train$Claim))  
w[C_1] <- 1 / (4*n1)  # Plus n1 est petit, plus le poids est grand
w[C_2] <- 1 / (4*n2)
w[C_3] <- 1 / (4*n3)
w[C_4] <- 1 / (4*n4)  # Classe rare => poids plus grand

model.complet <- lm(Claim~., data=train, weights=w)
summary(model.complet)$coefficients %>%
  round(4) %>%
  kbl() %>%
  kable_styling(full_width = FALSE,)
```

```{r}
y_pred <- predict(model.complet, test %>%  dplyr::select(-Claim))
metrics_complet <- tibble(
  RMSE_C = rmse_c(y_test, y_pred),
  RMSE = rmse(y_test, y_pred),
  R2 = R2(y_pred, y_test),
  MSE = mean((y_test - y_pred)^2),
)

metrics_complet %>%
  kbl(digits = 3) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```
```{r}
results <- data.frame(
  "pred" = y_pred,
  "actual" = y_test,
  "status" = y_test == round(y_pred)
)

ggplot(results, aes(x = actual, y = pred)) +
  geom_point(aes(color = status)) +
  labs(
    x = "Actual",
    y = "Prediction",
    title = "Répartitions des erreurs"
  ) +
  theme_minimal()
```

## Model B: Big model

On ajoute des intéractions entre des variables jugées importantes:
```{r}
# Variables numériques pertinentes pour produits
vars <- c("Period_Exp", "Car_Power", "Car_Age", "Age", "Bonus_Malus", "Inhab_density")

# Produits à deux variables (var1 * var2)
for (i in 1:(length(vars) - 1)) {
  for (j in (i + 1):length(vars)) {  # j commence à i+1 pour éviter répétitions
    var1 <- vars[i]
    var2 <- vars[j]
    
    new_var_name <- paste0(var1, "_x_", var2)  
    data[[new_var_name]] <- data[[var1]] * data[[var2]]
  }
}

# Produits à trois variables (var1 * var2 * var3)
for (i in 1:(length(vars) - 2)) {
  for (j in (i + 1):(length(vars) - 1)) {
    for (k in (j + 1):length(vars)) {  # k commence à j+1 pour éviter répétitions
      var1 <- vars[i]
      var2 <- vars[j]
      var3 <- vars[k]

      new_var_name <- paste0(var1, "_x_", var2, "_x_", var3)
      data[[new_var_name]] <- data[[var1]] * data[[var2]] * data[[var3]] 
    }
  }
}
```

Maintenant, on fait la même chose mais avec des ratios. A noter qu'on ne peut pas le faire pour les mêmes variables que précédemment car certaines valeurs peuvent être proches de 0:
```{r}
data %>% dplyr::select(c(
  "Period_Exp", "Car_Power", "Car_Age", "Age", "Bonus_Malus", "Inhab_density")) %>% summary()
```
On peut voir que `Car_Age`, `Inhab_density` et `Period_Exp` peuvent avoir des valeurs très petites, ce qui peut rendre instable la régression à cause du ratio. Donc on ne prends pas de risque et on les retire:

```{r}
# Variables à combiner pour ratio
vars <- c("Period_Exp", "Car_Power", "Age", "Bonus_Malus", "Inhab_density")

for (i in 1:length(vars)) {
  for (j in 1:length(vars)) {
    if (i != j) {  # On évite var1 / var1
      var1 <- vars[i]
      var2 <- vars[j]
      new_var_name <- paste0(var1, "_div_", var2)
      data[[new_var_name]] <- data[[var1]] / data[[var2]]
    }
  }
}
```

```{r}
data$Claim <- as.numeric(data$Claim)
data <- data %>%
  mutate(Ci = case_when(
    Claim %in% c(0, 1) ~ 1,
    Claim == 2 ~ 2,
    Claim == 3 ~ 3,
    Claim >= 4 ~ 4,
  ))
data$Ci <- as.factor(data$Ci)
train.index <- createDataPartition(data$Ci, p = .6, list = FALSE)
data <- data %>% dplyr::select(-Ci)

train <- data[ train.index,] 
test  <- data[-train.index,]
x_train <- train %>% 
  dplyr::select(-Claim) %>%
  as.matrix()
y_train <- train$Claim

x_test <- test %>% 
  dplyr::select(-Claim) %>%
  as.matrix()
y_test <- test$Claim

head(train)
```


```{r}
model.complet <- lm(Claim~., data=train, weights=w)
summary(model.complet)$coefficients %>%
  round(4) %>%
  kbl() %>%
  kable_styling(full_width = FALSE,)
```


```{r}
y_pred <- predict(model.complet, test %>%  dplyr::select(-Claim))
metrics_complet <- tibble(
  RMSE_C = rmse_c(y_test, y_pred),
  RMSE = rmse(y_test, y_pred),
  R2 = R2(y_pred, y_test),
  MSE = mean((y_test - y_pred)^2)
)

metrics_complet %>%
  kbl(digits = 3) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```

```{r}
results <- data.frame(
  "pred" = y_pred,
  "actual" = y_test,
  "status" = y_test == round(y_pred)
)

ggplot(results, aes(x = actual, y = pred)) +
  geom_point(aes(color = status)) +
  labs(
    x = "Actual",
    y = "Prediction",
    title = "Répartitions des erreurs"
  ) +
  theme_minimal()
```

## Model C: Ridge
```{r}
grid = 10^seq(4,-6,length=100)

x_train_pen <- train %>% 
  dplyr::select(-Claim) %>%
  dplyr::select_if(is.numeric) %>%
  as.matrix()
x_test_pen <- test %>% 
  dplyr::select(-Claim) %>%
  dplyr::select_if(is.numeric) %>%
  as.matrix()

model.lasso <- cv.glmnet(
  x_train_pen, y_train,
  alpha = 0, 
  lambda = grid, 
  preProc = c("center", "scale"),
  method = "glmnet",
  weights = w,
  trControl = custom)
```

Sur des données test, l’optimum peut être à $\lambda$ faible car le modèle a besoin d’une certaine complexité pour bien généraliser. A priori c'est un modèle dense.


```{r}
# Calcul de la RMSE combinée, biais² et variance
preds <- predict(model.lasso, newx = x_test_pen, s=grid)
rmse_c_values <- apply(preds, 2, function(p) rmse_c(y_test, p, FALSE))
biais2 <- (colMeans(preds) - mean(y_test))^2
variance <- apply(preds, 2, var)
rmse <- apply(preds, 2, function(p) rmse(y_test, p))

rmse_c_values_min <- min(rmse_c_values)
rmse_c_values_sd <- sd(rmse_c_values)
valid_indices <- which(rmse_c_values <= (rmse_c_values_min + rmse_c_values_sd / 10 ))
lambda_1se <- min(grid[valid_indices])
```


```{r}
# Normalisation des valeurs pour mettre sur une même échelle
df_plot <- data.frame(
  lambda = grid,
  RMSE = rmse/max(rmse),
  RMSE_C = rmse_c_values/(max(rmse_c_values)),
  Biais2 = biais2 / max(biais2),
  Variance = variance / max(variance)
) %>%
  mutate(lambda_log = log(lambda))

# Tracé du graphique avec ggplot2
ggplot(df_plot, aes(x = lambda_log)) +
  geom_line(aes(y = RMSE_C, color = "RMSE_C", linetype = "RMSE_C"), linewidth = 0.5) +
  geom_line(aes(y = RMSE, color = "RMSE", linetype = "RMSE"), linewidth = 0.5) +
  geom_line(aes(y = Biais2, color = "Biais²", linetype = "Biais²"), linewidth = 0.5) +
  geom_line(aes(y = Variance, color = "Variance", linetype = "Variance"), linewidth = 0.5) +
  scale_color_manual(values = c("RMSE_C" = "black", "RMSE" = "darkgray", 
                                "Biais²" = "blue", "Variance" = "red")) +
  scale_linetype_manual(values = c("RMSE_C" = "solid", "RMSE" = "solid", 
                                   "Biais²" = "dashed", "Variance" = "dashed"), guide = "none") + 
  labs(x = "log(Lambda)", y = "Normalized", 
       title = "Biais, Variance et RMSE_C normalisés en fonction de Lambda") +
  geom_vline(xintercept = log(lambda_1se), color = "lightgray") +
  theme_minimal()
```



```{r}
coef(model.lasso, s = lambda_1se) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "Variable") %>%
  rename(Coefficient = `s1`) %>%
  filter(round(Coefficient, 3) != 0) %>% 
  kbl(digits = 3) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```

```{r}
# Calcul des métriques
y_pred <- predict(model.lasso, newx = x_test_pen, s = lambda_1se)
metrics_lasso <- tibble(
  RMSE_C = rmse_c(y_test,y_pred),
  RMSE = rmse(y_test, y_pred),
  R2 = R2(y_pred, y_test),
  MSE = mean((y_test - y_pred)^2)
)

metrics_lasso %>%
  kbl(digits = 3) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```
```{r}
results <- data.frame(
  "pred" = as.vector(y_pred),
  "actual" = as.vector(y_test),
  "status" = as.vector(y_test == round(y_pred))
)

ggplot(results, aes(x = actual, y = pred)) +
  geom_point(aes(color = status)) +
  labs(
    x = "Actual",
    y = "Prediction",
    title = "Répartitions des erreurs"
  ) +
  theme_minimal()
```


## Model D: Regression linéaire sur les variables du LASSO
-> à faire <-

```{r}
C_1 <- which(train$Claim %in% c(0, 1))
C_2 <- which(train$Claim == 2)
C_3 <- which(train$Claim == 3)
C_4 <- which(train$Claim > 2)

n1 <- length(which(train$Claim %in% c(0, 1)))
n2 <- length(which(train$Claim == 2))
n3 <- length(which(train$Claim == 3))
n4 <- length(which(train$Claim > 3))

# Référence
n_max <- max(n1, n2, n3, n4)

# Calculer les poids pour équilibrer l'importance des classes
w <- numeric(length(train$Claim))  
w[C_1] <- n_max / n1  # Plus n1 est petit, plus le poids est grand
w[C_2] <- n_max / n2
w[C_3] <- n_max / n3
w[C_4] <- n_max / n4  # Classe rare => poids plus grand
```

```{r}
model.big <- lm(
  Claim ~ .
    + I(Bonus_Malus/Period_Exp) + I(Bonus_Malus*Period_Exp) 
    + I(Age*Bonus_Malus) + I(Age/Bonus_Malus)  
    + I(Age*Period_Exp) + I(Age/Period_Exp)
    + I(Period_Exp*Bonus_Malus*Age) + I(Period_Exp*Bonus_Malus/Age), 
  #preProc = c("center", "scale"),
  weights=w,
  data=train)

model.big <- lm(
  Claim ~ . + (Bonus_Malus + Period_Exp + Age)
              + I(Bonus_Malus / Period_Exp) + I(Age * Bonus_Malus / Period_Exp)
              + I(Period_Exp / Bonus_Malus) + I(Bonus_Malus * Period_Exp)
              + I(Age * Bonus_Malus) + I(Age * Period_Exp)
              + I(Age / Bonus_Malus) + I(Age / Period_Exp)
              + I(Bonus_Malus * Period_Exp * Age)
              + I(Bonus_Malus / (Period_Exp * Age))
              + I(Period_Exp / (Bonus_Malus * Age))
              + I(Age / (Bonus_Malus * Period_Exp)),
  weights=w,
  data=train)

summary(model.big)$coefficients %>%
  round(4) %>%
  kbl() %>%
  kable_styling(full_width = FALSE)
```

```{r}
y_pred <- predict(model.big, test)
metrics_big <- tibble(
  RMSE_C = rmse_c(y_test, y_pred),
  RMSE = rmse(y_test, y_pred),
  R2 = R2(y_pred, y_test),
  MSE = mean((y_test - y_pred)^2)
)

metrics_big %>%
  kbl(digits = 3) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```

```{r}
n <- dim(data)[1]
model.step <- stepAIC(model.big, Claim ~ ., trace=FALSE, k=log(n), direction="backward")
model.step
```


```{r}
summary(model.step)$coefficients %>%
  round(4) %>%
  kbl() %>%
  kable_styling(full_width = FALSE)
```

```{r}
y_pred <- predict(model.step, test %>%  dplyr::select(-Claim))
metrics_complet <- tibble(
  RMSE_C = rmse_c(y_test, y_pred),
  RMSE = rmse(y_test, y_pred),
  R2 = R2(y_pred, y_test),
  MSE = mean((y_test - y_pred)^2)
)

metrics_complet %>%
  kbl(digits = 3) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```

```{r}
results <- data.frame(
  "pred" = y_pred,
  "actual" = y_test,
  "status" = y_test == round(y_pred)
)

ggplot(results, aes(x = actual, y = pred)) +
  geom_point(aes(color = status)) +
  labs(
    x = "Actual",
    y = "Prediction",
    title = "Répartitions des erreurs"
  ) +
  theme_minimal()



ggplot(results, aes(x = factor(actual))) +  
  geom_point(aes(y = "pred"), fill = "steelblue", color = "black") +
  facet_wrap(~ actual, ncol=3) +
  labs(
    x = "Actual",
    y = "Prediction",
    title = "Répartitions des erreurs"
  ) +
  theme_minimal()
```

























