```{r}
# Charger les librairies nécessaires
library(dplyr)      # Manipulation des données
library(tidyr)      # Manipulation des données
library(ggplot2)    # Visualisation
library(readr)      # Lecture de fichiers CSV
library(GGally)
set.seed(2025)

# Charger les données
train <- read.csv('train_set.csv', header = T, sep = ",",dec=".")


# Afficher un aperçu des données
head(train)

```

```{r}
# Conversion des colonnes catégoriques en facteurs
categorical_cols <- train %>% select(where(is.character)) %>% colnames()
train[categorical_cols] <- lapply(train[categorical_cols], as.factor)
```

```{r}
library(Metrics)

rmse_c <- function(actuals, predictions) {
  # Définition des classes
  C_1 <- which(actuals %in% c(0, 1))
  C_2 <- which(actuals == 2)
  C_3 <- which(actuals == 3)
  C_4 <- which(actuals > 3)

  # Calcul du RMSE pour chaque classe (en évitant les erreurs si une classe est vide)
  rmse_1 <- rmse(actuals[C_1], predictions[C_1])
  rmse_2 <- rmse(actuals[C_2], predictions[C_2])
  rmse_3 <- rmse(actuals[C_3], predictions[C_3])
  rmse_4 <- rmse(actuals[C_4], predictions[C_4])

  # Combinaison des RMSE (en ignorant les valeurs NA)
  rmse_values <- c(rmse_1, rmse_2, rmse_3, rmse_4)
  RMSE_C <- mean(rmse_values, na.rm = TRUE)  # Moyenne des RMSE valides

  # Affichage des résultats
  cat("RMSE_1 (classe très fréquente) :", rmse_1, "\n")
  cat("RMSE_2 (classe fréquente) :", rmse_2, "\n")
  cat("RMSE_3 (classe rare) :", rmse_3, "\n")
  cat("RMSE_4 (classe très rare) :", rmse_4, "\n")
  cat("RMSE combiné (RMSE_C) :", RMSE_C, "\n")

  return(RMSE_C)
}

```

Data prep: Splitting 70/30 + ajout de la colonne Age_class + suppression de l'ID (ne sert à rien) + suppression de French_region + encodage ordinal (pour éviter les modalités nombreuses) +
```{r message=FALSE}
# Charger les bibliothèques nécessaires
library(dplyr)
library(lattice)
library(caret)
library(rsample)

# Créer une copie du dataset original pour ajouter des colonnes
train_modified <- train

# Ajouter la colonne 'Age_class'
train_modified <- train_modified %>%
  mutate(Age_class = case_when(
    Age < 25 ~ 0,
    Age >= 25 & Age <= 50 ~ 1,
    Age > 50 ~ 2
  ))

# Encodage ordinal des variables catégoriques
train_modified <- train_modified %>%
  mutate(
    Car_Model = as.numeric(factor(Car_Model, levels = unique(Car_Model))),
    Car_Fuel = as.numeric(factor(Car_Fuel, levels = unique(Car_Fuel))),
    Urban_rural_class = as.numeric(factor(Urban_rural_class, levels = unique(Urban_rural_class)))
  )

# Ajouter la colonne de stratification 'Claim_stratify'
train_modified <- train_modified %>%
  mutate(Claim_stratify = case_when(
    Claim == 0 ~ 1,
    Claim == 1 ~ 1,
    Claim == 2 ~ 2,
    Claim == 3 ~ 3,
    Claim > 3 ~ 4
  ))

# Séparer la classe 4
classe_4 <- train_modified %>% filter(Claim_stratify == 4)

# Séparer 2 observations pour le test et 4 pour le train
classe_4_test <- classe_4 %>% sample_n(2)
classe_4_train <- classe_4 %>% anti_join(classe_4_test, by = "PolID")  # pour ne pas avoir les mêmes éléments de la classe 4 dans le test et le train

# Split du reste des données avec stratification
reste <- train_modified %>% filter(Claim_stratify != 4)

split_reste <- initial_split(reste, prop = 0.7, strata = "Claim_stratify")

# Obtenir les sets train et test restants
train_reste <- training(split_reste)
test_reste <- testing(split_reste)

# Ajouter la classe 4 dans les splits finaux
train_set <- bind_rows(train_reste, classe_4_train)
test_set <- bind_rows(test_reste, classe_4_test)

# Supprimer la colonne de stratification après le split
train_set <- train_set %>% select(-Claim_stratify)
test_set <- test_set %>% select(-Claim_stratify)

# Supprimer certaines colonnes non pertinentes pour l'étude
train_set <- train_set %>% select(-French_region, -PolID)
test_set <- test_set %>% select(-French_region, -PolID)


# Standardiser les variables prédictives (exclure Claim)
train_set_scaled <- train_set %>%
  select(-Claim) %>%
  scale() %>%
  as.data.frame()

test_set_scaled <- test_set %>%
  select(-Claim) %>%
  scale() %>%
  as.data.frame()

# Réintégrer Claim après scaling
train_set_scaled$Claim <- train_set$Claim
test_set_scaled$Claim <- test_set$Claim

summary(test_set)
```

##Modélisation sans weights et sans régulairsation
Les modèles choisis sont des modèles de comptage (sauf le lm mais il sert uniquement de baseline)
```{r message=FALSE}
# Load necessary libraries
library(dplyr)
library(caret)
library(Metrics)
library(MASS)
library(tweedie)
library(statmod)
library(glmnet)
library(stats)
# Train and evaluate models

# 1. Linear Regression
lm_model <- lm(Claim ~ ., data = train_set_scaled)
lm_predictions <- predict(lm_model, newdata = test_set_scaled)
lm_predictions_class <- round(lm_predictions)  # Round to nearest integer for class prediction
rmse_lm <- rmse_c(test_set_scaled$Claim, lm_predictions_class)

# 2. Poisson GLM
glm_model <- glm(Claim ~ ., data = train_set_scaled, family = poisson(link = "log"))
glm_predictions <- predict(glm_model, newdata = test_set_scaled, type = "response")
glm_predictions_class <- round(glm_predictions)  # Round to nearest integer for class prediction
rmse_glm <- rmse_c(test_set_scaled$Claim, glm_predictions_class)

# 3. Negative Binomial GLM
negbin_model <- glm.nb(Claim ~ ., data = train_set_scaled)
negbin_predictions <- predict(negbin_model, newdata = test_set_scaled, type = "response")
negbin_predictions_class <- round(negbin_predictions)  # Round to nearest integer for class prediction
rmse_negbin <- rmse_c(test_set_scaled$Claim, negbin_predictions_class)

```

```{r}
# Display results
rmse_results <- data.frame(
  Model = c("Linear Regression", "Poisson GLM", "Negative Binomial GLM"),
  RMSE_C = c(rmse_lm, rmse_glm, rmse_negbin)
)

print(rmse_results)
```

```{r}
summary(test_set)
```

```{r}
summary(glm_predictions_class)
```

```{r}
summary(negbin_predictions_class)
```

```{r}
summary(negbin_model)
```

##Modélisation avec régularisation lasso et weights (le choix du lambda optimal est fait par cross-validation)
```{r}
# Load necessary libraries
library(dplyr)
library(caret)
library(Metrics)
library(MASS)
library(tweedie)
library(statmod)
library(glmnet)
library(stats)

# Assign weights to classes based on their frequency
class_weights <- 1 / table(train_set_scaled$Claim)  # Inverse of class frequency
weights <- class_weights[as.character(train_set_scaled$Claim)]  # Assign weights to each observation

# Convert dataframes to matrices
train_set_scaled_matrix <- as.matrix(train_set_scaled)
test_set_scaled_matrix <- as.matrix(test_set_scaled)

# Identify the column index of 'Claim'
claim_col_index <- which(colnames(train_set_scaled_matrix) == "Claim")

# Prepare data for glmnet
x_train <- train_set_scaled_matrix[, -claim_col_index]  # Predictors (training set)
y_train <- train_set_scaled_matrix[, claim_col_index]  # Target variable (training set)

x_test <- test_set_scaled_matrix[, -claim_col_index]  # Predictors (test set)
y_test <- test_set_scaled_matrix[, claim_col_index]  # Target variable (test set)

# Fit Lasso regression model with cross-validation
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, family = "poisson", weights = weights, nfolds = 5)

# Best lambda value
best_lambda <- lasso_cv$lambda.min

# Fit final Lasso model using the best lambda
lasso_model <- glmnet(x_train, y_train, alpha = 1, family = "poisson", lambda = best_lambda, weights = weights)

# Make predictions on the test set
lasso_predictions <- predict(lasso_model, newx = x_test, type = "response")
lasso_predictions_class <- round(lasso_predictions)  # Round to nearest integer for class prediction

# Evaluate performance
rmse_lasso <- rmse_c(y_test, lasso_predictions_class)

# Display results
cat("RMSE for Lasso Regression:", rmse_lasso, "\n")

# Compare with other models
rmse_results <- data.frame(
  Model = c("Linear Regression", "Poisson GLM", "Negative Binomial GLM", "Lasso Regression"),
  RMSE_C = c(rmse_lm, rmse_glm, rmse_negbin, rmse_lasso)
)

print(rmse_results)
```
```{r}
# Load necessary libraries
library(dplyr)
library(caret)
library(Metrics)
library(MASS)
library(statmod)
library(glmnet)
library(stats)

# Identifier les classes présentes dans train_set_scaled
present_classes <- unique(train_set_scaled$Claim)

# Définir les poids pour les classes présentes
class_weights <- c("0" = 1, "1" = 10, "2" = 100, "3" = 1000, "4" = 1e5, 
                   "5" = 1e5, "6" = 1e5, "7" = 1e5, "8" = 1e5, "9" = 1e5, 
                   "10" =1e5, "11" = 1e5)

# Filtrer les poids pour ne garder que les classes présentes
class_weights_present <- class_weights[as.character(present_classes)]

# Assigner les poids aux observations
weights <- class_weights_present[as.character(train_set_scaled$Claim)]

# Convert dataframes to matrices
train_set_scaled_matrix <- as.matrix(train_set_scaled)
test_set_scaled_matrix <- as.matrix(test_set_scaled)

# Identify the column index of 'Claim'
claim_col_index <- which(colnames(train_set_scaled_matrix) == "Claim")

# Prepare data for glmnet
x_train <- train_set_scaled_matrix[, -claim_col_index]  # Predictors (training set)
y_train <- train_set_scaled_matrix[, claim_col_index]  # Target variable (training set)

x_test <- test_set_scaled_matrix[, -claim_col_index]  # Predictors (test set)
y_test <- test_set_scaled_matrix[, claim_col_index]  # Target variable (test set)

# Fit Lasso regression model with cross-validation
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, family = "poisson", weights = weights, nfolds = 5)

# Best lambda value
best_lambda <- lasso_cv$lambda.min

# Fit final Lasso model using the best lambda
lasso_model <- glmnet(x_train, y_train, alpha = 1, family = "poisson", lambda = best_lambda, weights = weights)

# Make predictions on the test set
lasso_predictions <- predict(lasso_model, newx = x_test, type = "response")
lasso_predictions_class <- round(lasso_predictions)  # Round to nearest integer for class prediction

# Evaluate performance
rmse_lasso <- rmse_c(y_test, lasso_predictions_class)

# Display results
cat("RMSE for Lasso Regression:", rmse_lasso, "\n")

# Compare with other models
rmse_results <- data.frame(
  Model = c("Linear Regression", "Poisson GLM", "Negative Binomial GLM","Lasso Regression"),
  RMSE_C = c(rmse_lm, rmse_glm, rmse_negbin, rmse_lasso)
)

print(rmse_results)
```
```{r}

```

```{r}
# library(MASS)        # Pour la régression binomiale négative
# library(mpath)      # Pour la régression binomiale négative avec Lasso
# 
# # Conversion des datasets en matrices pour mpath
# x_train <- as.matrix(train_set_scaled[, -ncol(train_set_scaled)])
# y_train <- train_set_scaled[, ncol(train_set_scaled)]
# 
# x_test <- as.matrix(test_set_scaled[, -ncol(test_set_scaled)])
# 
# 
# # Régression binomiale négative avec Lasso et validation croisée
# formula <- Claim ~ Age + Car_Power + Car_Age + Bonus_Malus + Car_Model + Car_Fuel + Urban_rural_class + Inhab_density + Age_class
# cv_nb_lasso_model <- cv.glmreg(formula , data = train_set_scaled,x=x_train,y=y_train,family="negbin", weights = weights,alpha=1,nfolds=5,parallel=TRUE)
# 
# # Sélection du lambda optimal
# best_lambda <- cv_nb_lasso_model$lambda.min
# 
# # Entraînement du modèle final avec le meilleur lambda
# final_model_nb_lasso <- glmreg(
#   formula,
#   data=train_set_scaled,
#   weights=weights,
#   alpha=1,
#   lambda = best_lambda
# )
# 
# # Prédictions sur l'ensemble de test
# negbin_lasso_predictions_class <- round(predict(final_model_nb_lasso, newx = x_test, type = "response"))
# 
# # Evaluate performance
# rmse_lasso_negbin <- rmse_c(y_test, negbin_lasso_predictions_class)
# 
# # Display results
# cat("RMSE for Lasso Regression:", rmse_lasso_negbin, "\n")

```

```{r}
# library(mpath)
# # Identifier les classes présentes dans train_set_scaled
# present_classes <- unique(train_set_scaled$Claim)
# 
# # Définir les poids pour les classes présentes
# class_weights <- c("0" = 1, "1" = 10, "2" = 100, "3" = 1000, "4" = 1e4,
#                    "5" = 1e4, "6" = 1e4, "7" = 1e4, "8" = 1e4, "9" = 1e4,
#                    "10" =1e4, "11" = 1e4)
# 
# # Filtrer les poids pour ne garder que les classes présentes
# class_weights_present <- class_weights[as.character(present_classes)]
# 
# # Assigner les poids aux observations
# weights <- class_weights_present[as.character(train_set_scaled$Claim)]
# 
# # Convert dataframes to matrices
# train_set_scaled_matrix <- as.matrix(train_set_scaled)
# test_set_scaled_matrix <- as.matrix(test_set_scaled)
# 
# # Spécifier explicitement les variables prédictives
# formula <- Claim ~ Age + Car_Power + Car_Age + Bonus_Malus + Car_Model + Car_Fuel + Urban_rural_class + Inhab_density + Age_class
# 
# zip_lasso_model <- zipath(formula= formula , data = train_set_scaled, weights = weights,family="negbin",alpha=1)
# 
# # Faire des prédictions sur le test set
# zip_lasso_predictions <- predict(zip_lasso_model, newdata = test_set_scaled, type = "response")
# zip_lasso_predictions_class <- round(zip_lasso_predictions)  # Arrondir à l'entier le plus proche
# 
# # Évaluer la performance
# rmse_zip_lasso <- rmse_c(y_test, zip_lasso_predictions_class)
# 
# # Afficher les résultats
# cat("RMSE for Zero-Inflated Poisson Lasso Regression:", rmse_zip_lasso, "\n")
```

##Zero Inflated model: je comprends pas pk les perfs sont égales au modèle linéaire
```{r}
library(pscl)
Formula <- Claim ~ Age + Car_Power + Car_Age + Bonus_Malus + Car_Model + Car_Fuel + Urban_rural_class + Inhab_density + Age_class
mod_zip <- pscl::zeroinfl(
  formula=Formula,
  data = train_set_scaled,
  dist="negbin"
)

zip_lasso_predictions <- predict(mod_zip, newdata = test_set_scaled, type = "response")
zip_lasso_predictions_class <- round(zip_lasso_predictions)

rmse_zip_lasso <- rmse_c(y_test, zip_lasso_predictions_class)
cat("RMSE for Zero-Inflated Poisson Lasso Regression:", rmse_zip_lasso, "\n")

```
```{r}
Formula <- Claim ~ .|.
mod_zip <- pscl::zeroinfl(
  formula=Formula,
  data = train_set_scaled,
  dist="negbin"
)

zip_lasso_predictions <- predict(mod_zip, newdata = test_set_scaled, type = "response")
zip_lasso_predictions_class <- round(zip_lasso_predictions)

rmse_zip_lasso <- rmse_c(y_test, zip_lasso_predictions_class)
cat("RMSE for Zero-Inflated Poisson Lasso Regression:", rmse_zip_lasso, "\n")
```







